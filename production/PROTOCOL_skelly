#!/bin/bash
#SBATCH --time=(RUNTIME):00:00
#SBATCH --partition=(PART)
#SBATCH --qos=(QOS)
#SBATCH --ntasks-per-socket=2 --gres=gpu:gtx:2 --tasks=2
#SBATCH --constraint="titanx"
#SBATCH --gres-flags=enforce-binding
#SBATCH --output=(REV)slurm.out --open-mode=append
#SBATCH --mail-user=robin@robinbetz.com
#SBATCH --job-name=(NAM)_prod_(REP)
#SBATCH --dependency=(DEPS)
#
#
#=====================================================================
#                            PROTOCOL(REV)
#=====================================================================
# GOAL   : Simulate
# INPUTS :
# OUTPUT : 
# PROJECT: (NAM)
# PATH   : (DIR) 
# DATE   : (NOW)
#=====================================================================
#

# Exit if any command fails
set -e

# Protocol revision number
rev="(REV)"
rep="(REP)"

# Directory containing mdin files (one directory up)
inpdir="(INP)"
cd "$inpdir/$rep"

# Symlinked the output from previous run to be the input here,
# and symlinked the prmtop from the preparation step
prmtop="(PRMTOP)"
ref="(REF)"

# Load necessary modules
date
source "/share/PI/rondror/software/amber_dev/setup_amber.sh"

# Check for P2P GPUs. Fail if not
p2p=$((P2P))
if [[ "$p2p" == *"NO"* ]]; then
  echo "ERROR! Didn't get P2P GPUs!"
  echo "HOSTNAME             = $HOSTNAME"
  echo "CUDA VISIBLE DEVICES = $CUDA_VISIBLE_DEVICES"
  exit 1
fi

# Do last equilibration step in each replicate
# Remove restraints entirely, equilibrate for 5ns
# in the NPT ensemble, 2.5fs timestep
if [[ ! -f "Eq_6.rst" ]]; then
    rst="(EQDIR)/Eq_5.rst"
    echo "No restraint equilibration for initial run: $SLURM_JOB_NAME"
    $MPI_HOME/bin/mpirun -np 2 --bind-to socket $AMBERHOME/bin/pmemd.cuda.MPI -O -i "$inpdir/Eq_6.mdin" \
                         -o "Eq_6.mdout" -p "$prmtop" -c "$rst" \
                         -r "Eq_6.rst" -ref "$ref" -x "Eq_6.nc"
    chmod a-w "Eq_6"*
fi

# Simulate at 310K in the NPT ensemble with 2.0fs timestep
# This run will probably run out of walltime.
if [[ ! -f "Prod_0.rst" ]]; then
    last=0
    $MPI_HOME/bin/mpirun -np 2 --bind-to socket $AMBERHOME/bin/pmemd.cuda.MPI -O -i "$inpdir/Prod_(RUNTIME)h.mdin" \
                         -o "Prod_${last}.mdout" -p "$prmtop" -c "Eq_6.rst" \
                         -r "Prod_${last}.rst" -x "Prod_${last}.nc" -ref "$ref"
    chmod a-w "Prod_0"*
fi

# Given final equilibration and initial production run, continue from here
last=$(ls -1 $inpdir/$rep/Prod*rst | sed -E -e "s@Prod_?@@" -e "s@.rst@@" -e "s@$inpdir/$rep/@@g" | sort -n | tail -n 1)
rst="$inpdir/$rep/Prod_${last}.rst"

# Remove write permissions for the previously completed trajectory
# Only if it completed successfully
if [[ -f "Prod_${last}.rst" && -s "Prod_${last}.rst" ]]; then
    chmod a-w "Prod_${last}"*
fi

# Now start working on the next trajectory
last=$((last+1))
echo "Next trajectory number for $SLURM_JOB_NAME is: $last"

# Simulate at 310K in the NPT ensemble with 2.0fs timestep
# This run will probably run out of walltime.
echo "Beginning run: $SLURM_JOB_NAME"
$MPI_HOME/bin/mpirun -np 2 --bind-to socket $AMBERHOME/bin/pmemd.cuda.MPI -O -i "$inpdir/Prod_(RUNTIME)h.mdin" \
                     -o "Prod_${last}.mdout" -p "$prmtop" -c "$rst" \
                     -r "Prod_${last}.rst" -x "Prod_${last}.nc" -ref "$ref"

# Total equilibration now performed.
echo "Done with run: $SLURM_JOB_NAME"

